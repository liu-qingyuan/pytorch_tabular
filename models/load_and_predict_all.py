import pandas as pd
import numpy as np
from pytorch_tabular import TabularModel
from torchmetrics import AUROC, Accuracy  # ä¿®æ­£ï¼šç›´æ¥ä» torchmetrics å¯¼å…¥
import joblib
import os
from sklearn.model_selection import StratifiedKFold
import torch
import json

def set_seed(seed=42):
    """Set all random seeds for reproducibility"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def get_metrics_from_result(metrics_result):
    """ä»è¯„ä¼°ç»“æœä¸­æå–æŒ‡æ ‡"""
    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
    if isinstance(metrics_result, list) and len(metrics_result) > 0:
        metrics_result = metrics_result[0]
    
    # å¦‚æœå·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥è¿”å›éœ€è¦çš„æŒ‡æ ‡
    if isinstance(metrics_result, dict):
        return {
            'test_accuracy': metrics_result['test_accuracy'],
            'test_auroc': metrics_result['test_auroc']
        }
    
    # å¦åˆ™è§£æè¡¨æ ¼æ ¼å¼
    lines = str(metrics_result).split('\n')
    metrics_dict = {}
    for line in lines:
        if 'â”‚' not in line:  # Skip lines without table separator
            continue
        columns = line.split('â”‚')
        if len(columns) < 3:  # Skip invalid lines
            continue
        metric_name = columns[1].strip()
        if metric_name == 'test_accuracy':
            metrics_dict['test_accuracy'] = float(columns[2].strip())
        elif metric_name == 'test_auroc':
            metrics_dict['test_auroc'] = float(columns[2].strip())
    
    if not metrics_dict:
        raise ValueError(f"æ— æ³•ä»ç»“æœä¸­è§£æå‡ºæŒ‡æ ‡:\n{metrics_result}")
    
    return metrics_dict

def load_model_and_predict(model_name, data_path, fold="ensemble"):
    """åŠ è½½æ¨¡å‹è¿›è¡Œé¢„æµ‹
    
    Args:
        model_name: æ¨¡å‹åç§° (DANetModel/TabTransformerModel/AutoIntModel/TabNetModel)
        data_path: æ•°æ®è·¯å¾„
        fold: æŒ‡å®šfoldç¼–å·(1-10) æˆ– "ensemble"ï¼ˆé›†æˆé¢„æµ‹ï¼‰
        
    Returns:
        tuple: (predictions_df, fold_metrics_df) å¦‚æœæ˜¯ensembleæ¨¡å¼
        DataFrame: predictions_df å¦‚æœæ˜¯å•foldæ¨¡å¼
    """
    print("\n1. åŠ è½½æ•°æ®å’Œé¢„å¤„ç†é…ç½®...")
    # 1. è®¾ç½®åŸºç¡€éšæœºç§å­
    set_seed(42)
    
    # 2. åŠ è½½é¢„å¤„ç†é…ç½®
    scaler = joblib.load("results/scaler.pkl")
    print("åŠ è½½çš„scalerå‡å€¼æ ·ä¾‹:", scaler.mean_[:3])
    print("åŠ è½½çš„scaleræ–¹å·®æ ·ä¾‹:", scaler.var_[:3])
    
    # 3. åŠ è½½åŸå§‹æ•°æ®ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
    raw_df = pd.read_excel(data_path) if data_path.endswith('.xlsx') else pd.read_csv(data_path)
    
    # 4. è·å–ç‰¹å¾åˆ—ï¼ˆä¿æŒä¸è®­ç»ƒæ—¶ç›¸åŒçš„é¡ºåºï¼‰
    features = [c for c in raw_df.columns if c.startswith("Feature")]  # ä¸æ’åºï¼Œä¿æŒåŸå§‹é¡ºåº
    if not features:
        raise ValueError(f"æ•°æ®ä¸­æ²¡æœ‰æ‰¾åˆ°ç‰¹å¾åˆ—ï¼ˆä»¥'Feature'å¼€å¤´çš„åˆ—ï¼‰")
    print(f"åŠ è½½æ•°æ®: {len(raw_df)} è¡Œ, {len(features)} ä¸ªç‰¹å¾")
    print("ç‰¹å¾åˆ—:", features[:5], "...")  # æ‰“å°å‰å‡ ä¸ªç‰¹å¾åˆ—åï¼Œç”¨äºéªŒè¯
    
    # 5. æŒ‰åŸå§‹é¡ºåºæå–ç‰¹å¾å¹¶æ ‡å‡†åŒ–ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    X = raw_df[features].copy()
    y = raw_df["Label"].copy() if "Label" in raw_df.columns else None
    
    # 6. ä½¿ç”¨ä¿å­˜çš„scalerè¿›è¡Œæ ‡å‡†åŒ–ï¼ˆåœ¨åˆ’åˆ†foldä¹‹å‰ï¼‰
    try:
        X = pd.DataFrame(scaler.transform(X), columns=features)
        print("âœ“ å®Œæˆæ•°æ®æ ‡å‡†åŒ–")
    except ValueError as e:
        print("âŒ æ ‡å‡†åŒ–å¤±è´¥ï¼æ£€æŸ¥ç‰¹å¾åˆ—é¡ºåº...")
        print("æœŸæœ›çš„ç‰¹å¾é¡ºåº:", features)
        print("Scalerçš„ç‰¹å¾æ•°é‡:", len(scaler.mean_))
        raise
    
    # 7. ä½¿ç”¨ç›¸åŒçš„äº¤å‰éªŒè¯åˆ’åˆ†ï¼ˆåœ¨æ ‡å‡†åŒ–åçš„æ•°æ®ä¸Šåˆ’åˆ†ï¼‰
    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    # 8. è·å–æ‰€æœ‰foldçš„é¢„æµ‹ç»“æœ
    all_fold_preds = np.zeros(len(X))
    fold_weights = np.zeros(len(X))
    
    def get_probabilities(pred_df):
        """ä»é¢„æµ‹ç»“æœä¸­è·å–æ¦‚ç‡å€¼"""
        if isinstance(pred_df, pd.DataFrame):
            if 'target_1_probability' in pred_df.columns:
                return pred_df['target_1_probability'].values
            elif pred_df.shape[1] == 2 and all(c.endswith('probability') for c in pred_df.columns):
                return pred_df.iloc[:, 1].values
            else:
                raise ValueError(f"æ— æ³•è§£æé¢„æµ‹ç»“æœï¼Œåˆ—å: {pred_df.columns.tolist()}")
        else:
            raise ValueError("é¢„æµ‹ç»“æœå¿…é¡»æ˜¯DataFrameæ ¼å¼")
    
    print("\n2. å¼€å§‹æ¨¡å‹é¢„æµ‹...")
    if fold == "ensemble":
        # å¯¹æ¯ä¸ªfoldè¿›è¡Œé¢„æµ‹
        fold_metrics = []
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y), 1):
            try:
                # ä¸ºæ¯ä¸ªfoldè®¾ç½®ä¸åŒçš„éšæœºç§å­ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
                set_seed(42 + fold_idx)
                
                # åŠ è½½å½“å‰foldçš„æ¨¡å‹
                model_path = f"results/{model_name}_fold{fold_idx}"
                print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
                
                model = TabularModel.load_model(model_path)
                print("æ¨¡å‹ç±»åˆ«:", type(model.model))
                
                # å‡†å¤‡éªŒè¯æ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
                X_val = X.iloc[val_idx]
                val_df = X_val.copy()
                if y is not None:
                    val_df['target'] = y.iloc[val_idx]
                
                # å¯¹éªŒè¯é›†è¿›è¡Œé¢„æµ‹
                pred = model.predict(val_df)
                probs = get_probabilities(pred)
                
                # å°†é¢„æµ‹ç»“æœæ”¾å…¥å¯¹åº”ä½ç½®
                all_fold_preds[val_idx] = probs
                fold_weights[val_idx] = 1
                
                # è®¡ç®—å½“å‰foldçš„æ€§èƒ½æŒ‡æ ‡
                if y is not None:
                    # ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„è¯„ä¼°æ–¹æ³•
                    metrics_result = model.evaluate(val_df)
                    metrics = get_metrics_from_result(metrics_result)
                    fold_metrics.append({
                        'fold': fold_idx,
                        'samples': len(val_idx),
                        'auc': metrics['test_auroc'],
                        'acc': metrics['test_accuracy']
                    })
                
                print(f"âœ… Fold {fold_idx} é¢„æµ‹å®Œæˆ")
                # print(f"éªŒè¯é›†å¤§å°: {len(val_idx)}")
                # print(f"é¢„æµ‹å€¼èŒƒå›´: [{probs.min():.4f}, {probs.max():.4f}]")
                # print(f"é¢„æµ‹å€¼å‡å€¼: {probs.mean():.4f}")
                if y is not None:
                    print(f"AUC: {metrics['test_auroc']:.4f}, ACC: {metrics['test_accuracy']:.4f}")
                
            except Exception as e:
                print(f"âŒ Fold {fold_idx} é¢„æµ‹å¤±è´¥: {str(e)}")
                raise
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½æœ‰é¢„æµ‹å€¼
        if not np.all(fold_weights > 0):
            raise ValueError("æŸäº›æ ·æœ¬æ²¡æœ‰å¾—åˆ°é¢„æµ‹å€¼ï¼Œè¯·æ£€æŸ¥äº¤å‰éªŒè¯åˆ’åˆ†")
        
        print("\n3. é¢„æµ‹å®Œæˆ")
        
        if fold_metrics:
            print("\nğŸ“Š å„Foldæ€§èƒ½æ±‡æ€»:")
            fold_df = pd.DataFrame(fold_metrics)
            print(fold_df.to_string(index=False))
            print("\nå¹³å‡æ€§èƒ½:")
            print(f"Mean AUC: {fold_df['auc'].mean():.4f} Â± {fold_df['auc'].std():.4f}")
            print(f"Mean ACC: {fold_df['acc'].mean():.4f} Â± {fold_df['acc'].std():.4f}")
        
        # è¿”å›é¢„æµ‹ç»“æœå’Œfoldæ€§èƒ½æŒ‡æ ‡
        predictions_df = pd.DataFrame({
            'probability': all_fold_preds,
            'prediction': (all_fold_preds > 0.5).astype(int)
        })
        
        if fold_metrics:
            fold_metrics_df = pd.DataFrame(fold_metrics)
            return predictions_df, fold_metrics_df
        return predictions_df
    else:
        # å•ä¸ªfoldçš„é¢„æµ‹
        fold = int(fold)
        if not 1 <= fold <= 10:
            raise ValueError("foldå¿…é¡»åœ¨1-10ä¹‹é—´")
        
        # è®¾ç½®å¯¹åº”foldçš„éšæœºç§å­
        set_seed(42 + fold)
        
        # è·å–æŒ‡å®šfoldçš„éªŒè¯é›†ç´¢å¼•
        for fold_idx, (_, val_idx) in enumerate(kfold.split(X, y), 1):
            if fold_idx == fold:
                break
        else:
            raise ValueError(f"æ‰¾ä¸åˆ°fold {fold}")
        
        # åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹
        try:
            model_path = f"results/{model_name}_fold{fold}"
            print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
            
            model = TabularModel.load_model(model_path)
            print("æ¨¡å‹ç±»åˆ«:", type(model.model))
            
            # åªå¯¹è¯¥foldçš„éªŒè¯é›†è¿›è¡Œé¢„æµ‹ï¼ˆå·²ç»æ ‡å‡†åŒ–è¿‡ï¼‰
            X_val = X.iloc[val_idx]
            pred = model.predict(X_val)
            probs = get_probabilities(pred)
            
            print("\n3. é¢„æµ‹å®Œæˆ")
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            if y is not None:
                # å‡†å¤‡éªŒè¯æ•°æ®
                val_df = pd.DataFrame({
                    'target': y.iloc[val_idx],
                    **{f: X_val[f] for f in features}
                })
                # ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„è¯„ä¼°æ–¹æ³•
                metrics_result = model.evaluate(val_df)
                metrics = get_metrics_from_result(metrics_result)
                print(f"AUC: {metrics['test_auroc']:.4f}, ACC: {metrics['test_accuracy']:.4f}")
            
            return pd.DataFrame({
                'probability': probs,
                'prediction': (probs > 0.5).astype(int)
            })
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
            raise

def calculate_metrics(y_true, y_pred_proba, model):
    """ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æŒ‡æ ‡è®¡ç®—æ–¹å¼"""
    try:
        # å‡†å¤‡æ•°æ®ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        val_df = pd.DataFrame()
        val_df['target'] = y_true  # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„ç›®æ ‡åˆ—å
        
        # ä½¿ç”¨0.0è€Œä¸æ˜¯0æ¥ç¡®ä¿æµ®ç‚¹æ•°ç±»å‹
        for i, feat in enumerate(model.config.continuous_cols, 1):
            val_df[feat] = np.zeros(len(y_true), dtype=np.float32)  # æ˜¾å¼æŒ‡å®šfloat32ç±»å‹
        
        # ä½¿ç”¨æ¨¡å‹çš„evaluateæ–¹æ³•ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒçš„è¯„ä¼°æ–¹å¼ï¼‰
        metrics_result = model.evaluate(val_df)
        metrics = get_metrics_from_result(metrics_result)
        
        # ä»metricså­—å…¸ä¸­è·å–ç»“æœ
        return metrics['test_auroc'], metrics['test_accuracy'], metrics.get('test_loss', None)
    except Exception as e:
        print(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}")
        print(f"y_true ç±»å‹: {type(y_true)}, å½¢çŠ¶: {getattr(y_true, 'shape', 'unknown')}")
        print(f"y_pred_proba ç±»å‹: {type(y_pred_proba)}, å½¢çŠ¶: {getattr(y_pred_proba, 'shape', 'unknown')}")
        return None, None, None

def verify_performance(model_name, data_path="data/AI4healthcare.xlsx"):
    """éªŒè¯æ¨¡å‹æ€§èƒ½ä¸€è‡´æ€§"""
    # åŠ è½½åŸå§‹æ•°æ®
    df = pd.read_excel(data_path) if data_path.endswith('.xlsx') else pd.read_csv(data_path)
    y_true = df["Label"].copy()
    
    # è·å–é¢„æµ‹ç»“æœ
    preds = load_model_and_predict(model_name, data_path)
    
    # æ‰“å°é¢„æµ‹åˆ†å¸ƒä¿¡æ¯
    print("\né¢„æµ‹åˆ†å¸ƒ:")
    print(preds['probability'].describe())
    print("\næ ‡ç­¾åˆ†å¸ƒ:")
    print(y_true.value_counts(normalize=True))
    
    # åŠ è½½ç¬¬ä¸€ä¸ªfoldçš„æ¨¡å‹ç”¨äºè¯„ä¼°
    try:
        model = TabularModel.load_model(f"results/{model_name}_fold1")
        # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æŒ‡æ ‡è®¡ç®—æ–¹å¼
        auc, acc, loss = calculate_metrics(y_true, preds['probability'], model)
        if auc is None:
            print("âŒ æ€§èƒ½éªŒè¯å¤±è´¥ï¼šæŒ‡æ ‡è®¡ç®—é”™è¯¯")
            return
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return
    
    # åŠ è½½å†å²æœ€ä½³ç»“æœ
    with open('results/best_configs.json', 'r') as f:
        historical_best = json.load(f)
    hist_metrics = historical_best[model_name]['performance']
    hist_loss = hist_metrics['loss']
    hist_auc = hist_metrics['auc']
    hist_acc = hist_metrics['accuracy']
    
    print(f"\nğŸ” {model_name} æ€§èƒ½éªŒè¯ç»“æœ:")
    print(f"å½“å‰ Loss: {loss:.4f} | å†å²æœ€ä½³ Loss: {hist_loss:.4f}")
    print(f"å½“å‰ AUC: {auc:.4f} | å†å²æœ€ä½³ AUC: {hist_auc:.4f}")
    print(f"å½“å‰ ACC: {acc:.4f} | å†å²æœ€ä½³ ACC: {hist_acc:.4f}")
    
    # æ£€æŸ¥æ€§èƒ½æ˜¯å¦æ¥è¿‘å†å²æœ€ä½³
    loss_diff = abs(loss - hist_loss) if loss is not None else float('inf')
    if loss_diff > 0.01:  # å…è®¸1%çš„å·®å¼‚
        print("\nâš ï¸ è­¦å‘Š: Lossä¸å†å²æœ€ä½³ç›¸å·®è¾ƒå¤§!")
        print(f"Losså·®å¼‚: {loss_diff:.4f}")
        print("å¯èƒ½çš„åŸå› :")
        print("1. æ•°æ®é¢„å¤„ç†ä¸ä¸€è‡´")
        print("2. äº¤å‰éªŒè¯åˆ’åˆ†ä¸åŒ¹é…")
        print("3. æ¨¡å‹åŠ è½½é”™è¯¯")
        print("4. ç‰¹å¾é¡ºåºä¸ä¸€è‡´")
        print("5. æŒ‡æ ‡è®¡ç®—æ–¹å¼ä¸ä¸€è‡´")
    else:
        print("\nâœ… æ€§èƒ½éªŒè¯é€šè¿‡: Lossä¸å†å²æœ€ä½³æ¥è¿‘")
    
    # ä¿å­˜è¯¦ç»†çš„éªŒè¯ç»“æœ
    result_df = pd.DataFrame({
        'true_label': y_true,
        'probability': preds['probability'],
        'prediction': preds['prediction'],
        'fold_assignment': np.zeros(len(y_true))  # è®°å½•æ¯ä¸ªæ ·æœ¬å±äºå“ªä¸ªfold
    })
    
    # è®°å½•foldåˆ†é…
    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    for fold_idx, (_, val_idx) in enumerate(kfold.split(df[sorted([c for c in df.columns if c.startswith("Feature")])], y_true), 1):
        result_df.loc[val_idx, 'fold_assignment'] = fold_idx
    
    # æ·»åŠ éªŒè¯ä¿¡æ¯
    result_df['model_name'] = model_name
    result_df['validation_time'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
    result_df['historical_best_loss'] = hist_loss
    result_df['historical_best_auc'] = hist_auc
    result_df['historical_best_acc'] = hist_acc
    result_df['current_loss'] = loss
    result_df['current_auc'] = auc
    result_df['current_acc'] = acc
    
    # ä¿å­˜ç»“æœ
    result_df.to_csv(f"results/{model_name}_predictions.csv", index=False)
    print(f"\nâœ… ä¿å­˜é¢„æµ‹ç»“æœåˆ° results/{model_name}_predictions.csv")
    
    # åˆ†ææ¯ä¸ªfoldçš„æ€§èƒ½
    print("\nğŸ“Š å„Foldæ€§èƒ½åˆ†æ:")
    fold_metrics = []
    for fold in range(1, 11):
        fold_mask = result_df['fold_assignment'] == fold
        if fold_mask.any():
            try:
                # åŠ è½½å¯¹åº”foldçš„æ¨¡å‹
                fold_model = TabularModel.load_model(f"results/{model_name}_fold{fold}")
                fold_auc, fold_acc, fold_loss = calculate_metrics(
                    result_df.loc[fold_mask, 'true_label'],
                    result_df.loc[fold_mask, 'probability'],
                    fold_model
                )
                if fold_auc is not None:
                    fold_metrics.append({
                        'fold': fold,
                        'samples': fold_mask.sum(),
                        'loss': fold_loss,
                        'auc': fold_auc,
                        'acc': fold_acc
                    })
            except Exception as e:
                print(f"âŒ Fold {fold} è¯„ä¼°å¤±è´¥: {str(e)}")
    
    if fold_metrics:
        fold_df = pd.DataFrame(fold_metrics)
        print(fold_df)
        
        # æ£€æŸ¥foldé—´çš„æ€§èƒ½å·®å¼‚
        loss_std = fold_df['loss'].std()
        if loss_std > 0.05:  # å¦‚æœfoldé—´lossæ ‡å‡†å·®è¶…è¿‡0.05
            print("\nâš ï¸ è­¦å‘Š: Foldé—´æ€§èƒ½å·®å¼‚è¾ƒå¤§!")
            print(f"Lossæ ‡å‡†å·®: {loss_std:.4f}")
    
    # ç»˜åˆ¶é¢„æµ‹åˆ†å¸ƒ
    plot_predictions(preds, result_df['fold_assignment'], save_path=f"results/{model_name}_prediction_distribution.png")

def batch_predict(model_name, data_path, batch_size=1024):
    """åˆ†æ‰¹é¢„æµ‹é¿å…å†…å­˜ä¸è¶³"""
    df = pd.read_excel(data_path) if data_path.endswith('.xlsx') else pd.read_csv(data_path)
    total_samples = len(df)
    
    print(f"\nå¼€å§‹æ‰¹é‡é¢„æµ‹ {total_samples} ä¸ªæ ·æœ¬...")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    all_preds = []
    for start_idx in range(0, total_samples, batch_size):
        end_idx = min(start_idx + batch_size, total_samples)
        batch_df = df.iloc[start_idx:end_idx]
        
        # ä¿å­˜ä¸´æ—¶æ‰¹æ¬¡æ•°æ®
        temp_batch_path = "temp_batch.csv"
        batch_df.to_csv(temp_batch_path, index=False)
        
        # é¢„æµ‹å½“å‰æ‰¹æ¬¡
        try:
            batch_pred = load_model_and_predict(model_name, temp_batch_path)
            all_preds.append(batch_pred)
            print(f"âœ“ å®Œæˆæ‰¹æ¬¡ {start_idx//batch_size + 1}/{(total_samples-1)//batch_size + 1}")
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {start_idx//batch_size + 1} é¢„æµ‹å¤±è´¥: {str(e)}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_batch_path):
            os.remove(temp_batch_path)
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ
    final_preds = pd.concat(all_preds, ignore_index=True)
    print(f"\nâœ… æ‰¹é‡é¢„æµ‹å®Œæˆ")
    return final_preds

class ModelMonitor:
    def __init__(self, log_file="results/model_monitor.csv"):
        self.log_file = log_file
        self.current_run = {}  # å­˜å‚¨å½“å‰è¿è¡Œçš„æ‰€æœ‰æ¨¡å‹æ€§èƒ½
        self.performance_log = []
        
        # åŠ è½½å·²æœ‰æ—¥å¿—ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if os.path.exists(log_file):
            self.performance_log = pd.read_csv(log_file).to_dict('records')
    
    def track_performance(self, model_name, fold_metrics, data_info=""):
        """è®°å½•æ¨¡å‹æ€§èƒ½
        
        Args:
            model_name: æ¨¡å‹åç§°
            fold_metrics: åŒ…å«foldæ€§èƒ½çš„DataFrameï¼Œéœ€è¦åŒ…å«'auc'å’Œ'acc'åˆ—
            data_info: æ•°æ®æè¿°ä¿¡æ¯
        """
        try:
            # è®°å½•å½“å‰æ¨¡å‹çš„æ€§èƒ½
            self.current_run[model_name] = {
                "timestamp": pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
                "data_info": data_info,
                "metrics": fold_metrics.to_dict('records')
            }
            
            # æ£€æŸ¥æ€§èƒ½å˜åŒ–
            self._check_performance_drift(model_name, fold_metrics)
                
        except Exception as e:
            print(f"âŒ æ€§èƒ½è·Ÿè¸ªå¤±è´¥ ({model_name}): {str(e)}")
    
    def _save_log(self):
        """ä¿å­˜æ€§èƒ½æ—¥å¿—"""
        # å°†å½“å‰è¿è¡Œçš„æ‰€æœ‰ç»“æœä¿å­˜åˆ°æ—¥å¿—
        records = []
        for model_name, data in self.current_run.items():
            for fold_metric in data['metrics']:
                record = {
                    "timestamp": data['timestamp'],
                    "model": model_name,
                    "fold": fold_metric['fold'],
                    "data_info": data['data_info'],
                    "auc": fold_metric['auc'],
                    "accuracy": fold_metric['acc'],
                    "samples": fold_metric['samples']
                }
                records.append(record)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        pd.DataFrame(records).to_csv(self.log_file, index=False)
    
    def _check_performance_drift(self, model_name, fold_metrics):
        """æ£€æŸ¥æ€§èƒ½æ¼‚ç§»"""
        mean_auc = fold_metrics['auc'].mean()
        std_auc = fold_metrics['auc'].std()
        
        # æ£€æŸ¥æ¯ä¸ªfoldçš„æ€§èƒ½æ˜¯å¦å¼‚å¸¸
        for _, row in fold_metrics.iterrows():
            if abs(row['auc'] - mean_auc) > 2 * std_auc:
                print(f"\nâš ï¸ {model_name} Fold {row['fold']} æ€§èƒ½å¼‚å¸¸:")
                print(f"AUC: {row['auc']:.4f}")
                print(f"å¹³å‡ AUC: {mean_auc:.4f} Â± {std_auc:.4f}")
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.current_run:
            print("æ²¡æœ‰å¯ç”¨çš„æ€§èƒ½è®°å½•")
            return
        
        # å°†æ‰€æœ‰å½“å‰è¿è¡Œçš„ç»“æœæ•´ç†æˆDataFrame
        records = []
        for model_name, data in self.current_run.items():
            for fold_metric in data['metrics']:
                record = {
                    "timestamp": data['timestamp'],
                    "model": model_name,
                    "fold": fold_metric['fold'],
                    "data_info": data['data_info'],
                    "auc": fold_metric['auc'],
                    "accuracy": fold_metric['acc'],
                    "samples": fold_metric['samples']
                }
                records.append(record)
        
        df = pd.DataFrame(records)
        
        # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
        model_stats = df.groupby('model').agg({
            'auc': ['mean', 'std', 'min', 'max'],
            'accuracy': ['mean', 'std', 'min', 'max'],
            'samples': 'sum'
        }).round(4)
        
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)
        print(model_stats)
        
        # ä¿å­˜ç»“æœ
        self._save_log()

def plot_predictions(preds, fold_assignment, save_path="results/prediction_distribution.png"):
    """å¯è§†åŒ–é¢„æµ‹åˆ†å¸ƒ"""
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(10, 6))
    plt.hist(preds['probability'], bins=50, alpha=0.7)
    plt.title("é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ")
    plt.xlabel("Probability")
    plt.ylabel("Count")
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    stats = preds['probability'].describe()
    info = f"Mean: {stats['mean']:.3f}\nStd: {stats['std']:.3f}\n"
    info += f"Min: {stats['min']:.3f}\nMax: {stats['max']:.3f}"
    plt.text(0.95, 0.95, info,
             transform=plt.gca().transAxes,
             verticalalignment='top',
             horizontalalignment='right',
             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # æ·»åŠ foldä¿¡æ¯
    plt.bar(range(len(preds['probability'])), fold_assignment, color='red', alpha=0.5)
    
    plt.savefig(save_path)
    plt.close()
    print(f"âœ… ä¿å­˜é¢„æµ‹åˆ†å¸ƒå›¾åˆ°: {save_path}")

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šéªŒè¯æ‰€æœ‰æ¨¡å‹æ€§èƒ½
    model_names = ["DANetModel", "TabTransformerModel", "AutoIntModel", "TabNetModel"]
    
    print("\nå¼€å§‹æ¨¡å‹éªŒè¯...")
    monitor = ModelMonitor()
    
    for model_name in model_names:
        try:
            # è·å–é¢„æµ‹ç»“æœå’Œfoldæ€§èƒ½
            preds = load_model_and_predict(model_name, "data/AI4healthcare.xlsx")
            
            # è®°å½•æ€§èƒ½åˆ°ç›‘æ§å™¨
            if isinstance(preds, tuple) and len(preds) == 2:
                predictions, fold_metrics = preds
                monitor.track_performance(model_name, fold_metrics, "å…¨é‡æ•°æ®éªŒè¯")
            
        except Exception as e:
            print(f"\nâŒ {model_name} éªŒè¯å¤±è´¥: {str(e)}")
    
    # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
    monitor.generate_report() 