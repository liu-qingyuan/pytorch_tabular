import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

import pandas as pd
import numpy as np
from pytorch_tabular import TabularModel
import joblib
import os
import json
from sklearn.metrics import roc_auc_score, accuracy_score

def verify_model_performance(model_name, predictions, test_df):
    """
    éªŒè¯æ¨¡å‹æ€§èƒ½æ˜¯å¦ä¸å†å²è®°å½•ä¸€è‡´
    
    Args:
        model_name: æ¨¡å‹åç§°
        predictions: æ¨¡å‹é¢„æµ‹ç»“æœ
        test_df: æµ‹è¯•æ•°æ®é›†
    """
    # åŠ è½½å†å²æœ€ä½³é…ç½®
    best_configs_file = 'results/best_configs.json'
    if not os.path.exists(best_configs_file):
        print(f"âŒ Historical best configurations file not found at {best_configs_file}")
        return
    
    with open(best_configs_file, 'r') as f:
        historical_best = json.load(f)
    
    if model_name not in historical_best:
        print(f"âŒ No historical record found for {model_name}")
        return
    
    # è·å–å†å²æ€§èƒ½
    hist_auc = historical_best[model_name]['performance']['auc']
    hist_acc = historical_best[model_name]['performance']['accuracy']
    
    # è®¡ç®—å½“å‰æ€§èƒ½
    # æ‰¾åˆ°æ¦‚ç‡åˆ—
    prob_cols = [col for col in predictions.columns if 'probability' in col]
    if not prob_cols:
        print(f"âŒ No probability column found in predictions. Available columns: {predictions.columns.tolist()}")
        return
    prob_col = prob_cols[0]
    
    # æ‰¾åˆ°é¢„æµ‹åˆ—ï¼ˆå¯èƒ½æ˜¯'prediction'æˆ–'target_prediction'ï¼‰
    pred_cols = [col for col in predictions.columns if 'prediction' in col and 'probability' not in col]
    if not pred_cols:
        print(f"âŒ No prediction column found in predictions. Available columns: {predictions.columns.tolist()}")
        # å¦‚æœæ²¡æœ‰é¢„æµ‹åˆ—ï¼Œæ ¹æ®æ¦‚ç‡ç”Ÿæˆé¢„æµ‹
        predictions['prediction'] = (predictions[prob_col] > 0.5).astype(int)
        pred_col = 'prediction'
    else:
        pred_col = pred_cols[0]
    
    current_auc = roc_auc_score(test_df['target'], predictions[prob_col])
    current_acc = accuracy_score(test_df['target'], predictions[pred_col])
    
    print(f"\nğŸ“Š Performance Comparison for {model_name}:")
    print("=" * 50)
    print(f"Metric    Current    Historical    Difference")
    print("-" * 50)
    print(f"AUC       {current_auc:.4f}    {hist_auc:.4f}        {current_auc - hist_auc:+.4f}")
    print(f"Accuracy  {current_acc:.4f}    {hist_acc:.4f}        {current_acc - hist_acc:+.4f}")
    print("=" * 50)

def load_model_and_predict(data_path, model_name, verify_performance=True):
    """
    åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
    
    Args:
        data_path: éœ€è¦é¢„æµ‹çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        model_name: æ¨¡å‹åç§° (DANetModel, TabTransformerModel, AutoIntModel, TabNetModel)
        verify_performance: æ˜¯å¦éªŒè¯æ¨¡å‹æ€§èƒ½
    
    Returns:
        predictions: é¢„æµ‹ç»“æœDataFrame
    """
    try:
        # 1. åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç›®å½•è·¯å¾„ï¼‰
        model_path = f"results/best_{model_name}"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model directory not found at {model_path}")
        
        model = TabularModel.load_model(model_path)
        print(f"\nğŸ“Š Model config for {model_name}:")
        print(f"Task: {model.config.task}")
        print(f"Metrics: {model.config.metrics}")
        
        # 2. åŠ è½½æ•°æ®
        df = pd.read_csv(data_path)
        
        # 3. ä»æ¨¡å‹é…ç½®ä¸­è·å–ç‰¹å¾åˆ—
        continuous_features = model.config.continuous_cols
        categorical_features = model.config.categorical_cols
        all_features = continuous_features + categorical_features
        
        print(f"\nç‰¹å¾åˆ—éªŒè¯:")
        print(f"è¿ç»­ç‰¹å¾ ({len(continuous_features)}): {continuous_features[:5]}...")
        print(f"ç±»åˆ«ç‰¹å¾ ({len(categorical_features)}): {categorical_features}")
        
        # éªŒè¯æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
        missing_features = [f for f in all_features if f not in df.columns]
        if missing_features:
            raise ValueError(f"Missing features in data: {missing_features}")
        
        if 'target' in df.columns:
            print("\nç›®æ ‡å˜é‡åˆ†å¸ƒ:")
            print(df['target'].value_counts(normalize=True))
        
        # 4. å‡†å¤‡è¾“å…¥æ•°æ®
        X = df[all_features].copy()
        
        # 5. åŠ è½½å’Œåº”ç”¨æ ‡å‡†åŒ–å™¨
        scaler_path = "results/scaler.pkl"
        if not os.path.exists(scaler_path):
            raise FileNotFoundError(f"Scaler file not found at {scaler_path}")
        
        scaler = joblib.load(scaler_path)
        
        # æ£€æŸ¥æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        print("\næ ‡å‡†åŒ–å‰ç»Ÿè®¡:")
        print(X[continuous_features].describe().round(3).loc[['mean', 'std']].head())
        
        # åªå¯¹è¿ç»­ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–
        X[continuous_features] = scaler.transform(X[continuous_features])
        
        print("\næ ‡å‡†åŒ–åç»Ÿè®¡:")
        print(X[continuous_features].describe().round(3).loc[['mean', 'std']].head())
        
        # 6. è¿›è¡Œé¢„æµ‹
        print("\nå¼€å§‹é¢„æµ‹...")
        predictions = model.predict(X)
        
        print("\né¢„æµ‹ç»“æœåˆ—:", predictions.columns.tolist())
        
        # 7. éªŒè¯æ¨¡å‹æ€§èƒ½ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if verify_performance and 'target' in df.columns:
            verify_model_performance(model_name, predictions, df)
        
        return predictions
        
    except Exception as e:
        print(f"âŒ Error with {model_name}:")
        import traceback
        traceback.print_exc()
        return None

def ensemble_predictions(predictions_list, model_names):
    """
    é›†æˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    
    Args:
        predictions_list: æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœåˆ—è¡¨
        model_names: å¯¹åº”çš„æ¨¡å‹åç§°åˆ—è¡¨
    
    Returns:
        ensemble_df: é›†æˆåçš„é¢„æµ‹ç»“æœ
    """
    if not predictions_list:
        print("âŒ No valid predictions to ensemble")
        return None
    
    print("\nğŸ”„ Creating ensemble predictions...")
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
    all_probs = []
    for preds in predictions_list:
        prob_col = [col for col in preds.columns if 'probability' in col][0]
        all_probs.append(preds[prob_col].values)
    
    # è®¡ç®—å¹³å‡æ¦‚ç‡
    avg_probs = np.mean(all_probs, axis=0)
    
    # åˆ›å»ºé›†æˆé¢„æµ‹ç»“æœ
    ensemble_df = pd.DataFrame({
        'ensemble_probability': avg_probs,
        'ensemble_prediction': (avg_probs > 0.5).astype(int)
    })
    
    # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
    for model_name, preds in zip(model_names, predictions_list):
        prob_col = [col for col in preds.columns if 'probability' in col][0]
        pred_col = 'prediction' if 'prediction' in preds.columns else None
        
        if prob_col:
            ensemble_df[f'{model_name}_probability'] = preds[prob_col]
        if pred_col:
            ensemble_df[f'{model_name}_prediction'] = preds[pred_col]
    
    # ä¿å­˜é›†æˆç»“æœ
    save_path = "results/ensemble_predictions.csv"
    ensemble_df.to_csv(save_path, index=False)
    print(f"âœ… Ensemble predictions saved to {save_path}")
    
    # æ‰“å°é›†æˆç»“æœç»Ÿè®¡
    print("\nğŸ“Š Ensemble Prediction Distribution:")
    print("\nProbability Distribution:")
    print(ensemble_df['ensemble_probability'].describe())
    print("\nPrediction Distribution:")
    print(ensemble_df['ensemble_prediction'].value_counts(normalize=True))
    
    return ensemble_df

if __name__ == "__main__":
    # åŠ è½½æµ‹è¯•é›†
    test_df = pd.read_csv("results/test_set.csv")
    print("\nğŸ“Š Verifying saved models performance...")
    
    # å®šä¹‰æ¨¡å‹åˆ—è¡¨
    model_names = ["DANetModel", "TabTransformerModel", "AutoIntModel", "TabNetModel"]
    
    # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œæ€§èƒ½éªŒè¯
    for model_name in model_names:
        try:
            predictions = load_model_and_predict(
                data_path="results/test_set.csv",  # ä½¿ç”¨ä¿å­˜çš„æµ‹è¯•é›†
                model_name=model_name,
                verify_performance=True  # å¼€å¯æ€§èƒ½éªŒè¯
            )
        except Exception as e:
            print(f"âŒ Failed to verify {model_name}: {str(e)}")
            continue 