# Suppress warnings
import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# Standard libraries
import os
import json
import copy
from pathlib import Path
from collections import namedtuple
from copy import deepcopy
from typing import Callable, Dict, Iterable, List, Optional, Union

# Data processing
import numpy as np
import pandas as pd
from pandas import DataFrame
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import (
    StratifiedKFold,
    ParameterSampler,
    ParameterGrid,
    BaseCrossValidator
)

# Deep learning
import torch
import torch.nn.functional as F
import joblib

# Progress tracking
from rich.progress import Progress

# PyTorch Tabular imports
from pytorch_tabular import TabularModel
from pytorch_tabular.config import (
    DataConfig,
    ModelConfig,
    OptimizerConfig,
    TrainerConfig
)
from pytorch_tabular.models import (
    TabNetModelConfig,
    DANetConfig,
    AutoIntConfig,
    TabTransformerConfig
)
from pytorch_tabular.models.common.heads import LinearHeadConfig
from pytorch_tabular.tabular_model import TabularModel
from pytorch_tabular.tabular_model_tuner import TabularModelTuner
from pytorch_tabular.utils import OOMException, OutOfMemoryHandler, get_logger, suppress_lightning_logs

# Get logger
logger = get_logger(__name__)

def multi_metric_agg(list_of_dicts):
    """
    å¤šæŒ‡æ ‡èšåˆå‡½æ•°ï¼šå°†å¤šæŠ˜çš„æŒ‡æ ‡ç»“æœèšåˆä¸ºä¸€ä¸ªå­—å…¸
    
    Args:
        list_of_dicts: å½¢å¦‚ [{"loss":0.5,"accuracy":0.8}, {"loss":0.6,"accuracy":0.81}, ...]
    
    Returns:
        dict: èšåˆåçš„ç»“æœï¼Œå¦‚ {"loss":0.55,"accuracy":0.805}
    """
    if not list_of_dicts:
        return {}
    # å–æ‰€æœ‰key
    keys = list_of_dicts[0].keys()
    agg_dict = {}
    for k in keys:
        vals = []
        for d in list_of_dicts:
            vals.append(d.get(k, np.inf))  # å¦‚æœOOMåˆ™å¯èƒ½æ˜¯np.inf
        agg_dict[k] = np.mean(vals)
    return agg_dict

class MultiMetricTabularModelTuner(TabularModelTuner):
    """æ”¯æŒå¤šæŒ‡æ ‡çš„TabularModelTuner
    
    ç»§æ‰¿TabularModelTunerï¼Œæ”¹å†™åœ¨CVåœºæ™¯ä¸‹å¦‚ä½•å¤„ç†å¤šæŒ‡æ ‡ã€‚
    å¯ä»¥æŒ‡å®šprimary_metricç”¨äºé€‰æœ€ä¼˜ï¼Œå…¶å®ƒæŒ‡æ ‡ä¹Ÿä¼šè¢«è®°å½•ã€‚
    """
    def __init__(
        self,
        primary_metric: str = "loss",    # ç”¨äºæœ€ç»ˆé€‰æœ€ä¼˜çš„æŒ‡æ ‡
        *args, **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.primary_metric = primary_metric

    def tune(
        self,
        train,
        search_space,
        metric,      # è¿™é‡Œå¿…é¡»æ˜¯ä¸€ä¸ªèƒ½è¿”å›dictçš„å¯è°ƒç”¨ï¼šcompute_metrics
        mode: str,
        validation=None,
        n_trials=None,
        cv=None,
        cv_kwargs={},
        return_best_model=True,
        verbose=False,
        progress_bar=True,
        random_state=42,
        ignore_oom=True,
        **kwargs,
    ):
        """æ”¯æŒå¤šæŒ‡æ ‡çš„è¶…å‚æ•°è°ƒä¼˜
        
        åŸºæœ¬æµç¨‹å’ŒåŸTabularModelTunerä¸€è‡´ï¼Œä½†åœ¨cvæ¨¡å¼ä¸‹:
        1) cross_validateç›´æ¥è¿”å›å¤šæŠ˜å¹³å‡åçš„dictç»“æœ
        2) æŠŠè¯¥dictä¸­æ‰€æœ‰keyéƒ½æ›´æ–°åˆ°params
        3) ç”¨ self.primary_metric æ¥åˆ¤æ–­æœ€ä¼˜
        """
        assert mode in ["max", "min"], "mode must be one of ['max', 'min']"
        if self.suppress_lightning_logger:
            from pytorch_tabular.utils import suppress_lightning_logs
            suppress_lightning_logs()

        if cv is not None and validation is not None:
            warnings.warn("Both validation and cv are provided. We'll use CV and ignore validation.")
            validation = None

        # å¦‚æœåªä¼ äº†ä¸€ä¸ªmodel_configå’Œä¸€ä¸ªsearch_spaceï¼Œä¹Ÿè¦åŒ…æˆlistç»Ÿä¸€å¤„ç†
        if not isinstance(self.model_config, list):
            model_configs = [self.model_config]
        else:
            model_configs = self.model_config

        if not isinstance(search_space, list):
            search_space = [search_space]

        trials = []
        best_model = None
        best_score = float("inf") if mode == "min" else float("-inf")

        with Progress() as progress_bar_inst:
            # å¯èƒ½æœ‰å¤šä¸ªmodel_config
            for idx, (mconfig_i, sspace_i) in enumerate(zip(model_configs, search_space)):

                # ç»™paramsé‡ŒåŠ ä¸€ä¸ªmodelå­—æ®µï¼ŒåŒºåˆ†ä¸åŒæ¨¡å‹
                sspace_i = {"model": [f"{idx}-{mconfig_i.__class__.__name__}"], **sspace_i}

                # æ ¹æ®æœç´¢ç­–ç•¥
                strategy = kwargs.pop("strategy", "random_search")
                if strategy == "grid_search":
                    iterator = list(ParameterGrid(sspace_i))
                    if n_trials is not None:
                        warnings.warn("n_trials is ignored for grid_search.")
                else:
                    # é»˜è®¤random_search
                    if n_trials is None:
                        raise ValueError("Need n_trials for random_search")
                    iterator = list(ParameterSampler(sspace_i, n_iter=n_trials, random_state=random_state))

                # å¦‚æœè¦æ˜¾ç¤ºè¿›åº¦æ¡
                if progress_bar:
                    iterator = progress_bar_inst.track(iterator, description=f"Model {idx} Searching")

                # datamoduleåªéœ€è¦åˆå§‹åŒ–ä¸€æ¬¡
                datamodule = None

                for i, params in enumerate(iterator):
                    # å¤åˆ¶é…ç½®
                    trainer_config_t = copy.deepcopy(self.trainer_config)
                    optimizer_config_t = copy.deepcopy(self.optimizer_config)
                    model_config_t = copy.deepcopy(mconfig_i)

                    # ç”¨_tunerçš„_update_configså‡½æ•°å†™å…¥config
                    optimizer_config_t, model_config_t = self._update_configs(
                        optimizer_config_t, model_config_t, params
                    )

                    # åˆå§‹åŒ–TabularModel
                    tabular_model_t = TabularModel(
                        data_config=self.data_config,
                        model_config=model_config_t,
                        optimizer_config=optimizer_config_t,
                        trainer_config=trainer_config_t,
                        **self.tabular_model_init_kwargs,
                    )

                    # æ„å»ºdatamodule
                    if not datamodule:
                        prep_dl_kwargs, prep_model_kwargs, train_kwargs = tabular_model_t._split_kwargs(kwargs)
                        if "seed" not in prep_dl_kwargs:
                            prep_dl_kwargs["seed"] = random_state
                        datamodule = tabular_model_t.prepare_dataloader(
                            train=train,
                            validation=validation,
                            **prep_dl_kwargs
                        )
                        valid_df = validation if validation is not None else datamodule.validation_dataset.data
                    else:
                        prep_model_kwargs, train_kwargs = {}, {}

                    # ========================
                    # è¿›å…¥CVæµç¨‹
                    # ========================
                    if cv is not None:
                        cv_verbose = cv_kwargs.pop("verbose", False)
                        cv_kwargs.pop("handle_oom", None)
                        with OutOfMemoryHandler(handle_oom=True) as handler:
                            # cross_validate ç›´æ¥è¿”å›å¹³å‡åçš„dict
                            cv_dict, _ = tabular_model_t.cross_validate(
                                cv=cv,
                                train=train,
                                metric=metric,  # compute_metrics(y_true, y_pred) => dict
                                verbose=cv_verbose,
                                handle_oom=False,
                                **cv_kwargs
                            )
                        if handler.oom_triggered:
                            # OOMå¤„ç†
                            if not ignore_oom:
                                raise OOMException("OOM happened!")
                            else:
                                # ç”¨infæ›¿ä»£
                                cv_dict = {self.primary_metric: np.inf}
                                params.update({"model": f"{params['model']} (OOM)"})

                    # ========================
                    # å¦åˆ™èµ° å¸¸è§„train+evaluate
                    # ========================
                    else:
                        model = tabular_model_t.prepare_model(
                            datamodule=datamodule,
                            **prep_model_kwargs
                        )
                        with OutOfMemoryHandler(handle_oom=True) as handler:
                            tabular_model_t.train(
                                model=model,
                                datamodule=datamodule,
                                handle_oom=False,
                                **train_kwargs
                            )
                        if handler.oom_triggered:
                            if not ignore_oom:
                                raise OOMException("OOM happened!")
                            else:
                                cv_dict = {self.primary_metric: np.inf}
                                params.update({"model": f"{params['model']} (OOM)"})
                        else:
                            # predictéªŒè¯é›† => è®¡ç®—å¤šæŒ‡æ ‡
                            preds = tabular_model_t.predict(valid_df)
                            y_true = valid_df[tabular_model_t.config.target]
                            cv_dict = metric(y_true, preds)  # dict

                    # æŠŠå¤šæŒ‡æ ‡éƒ½å†™åˆ°params
                    for k, v in cv_dict.items():
                        params[k] = v

                    # ============åˆ¤æ–­æ˜¯å¦æ˜¯æ›´ä¼˜æ¨¡å‹==========
                    current_score = cv_dict.get(self.primary_metric, np.inf if mode == "min" else -np.inf)
                    if best_model is None:
                        best_model = copy.deepcopy(tabular_model_t)
                        best_score = current_score
                    else:
                        if mode == "min" and current_score < best_score:
                            best_model = copy.deepcopy(tabular_model_t)
                            best_score = current_score
                        elif mode == "max" and current_score > best_score:
                            best_model = copy.deepcopy(tabular_model_t)
                            best_score = current_score

                    params["trial_id"] = i
                    trials.append(params)

                    if verbose:
                        metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in cv_dict.items()])
                        print(f"[Trial {i}] {params['model']} => {metrics_str}")

        # å…¨éƒ¨ç»“æŸåï¼Œæ±‡æ€»åˆ°DataFrame
        trials_df = pd.DataFrame(trials)
        trials = trials_df.pop("trial_id")

        # æ‰¾åˆ°æœ€ä½³è¡Œ
        if mode == "max":
            best_idx = trials_df[self.primary_metric].idxmax()
        else:
            best_idx = trials_df[self.primary_metric].idxmin()

        best_params = trials_df.iloc[best_idx].to_dict()
        best_score = best_params.pop(self.primary_metric)
        trials_df.insert(0, "trial_id", trials)

        if verbose:
            logger.info("Model Tuner Finished")
            metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in best_params.items() if k in cv_dict.keys()])
            logger.info(f"Best Model: {best_params['model']} - Metrics: {metrics_str}")

        if return_best_model and best_model is not None:
            # æŠŠdatamoduleæŒ‚å›å»
            best_model.datamodule = datamodule
            return self.OUTPUT(trials_df, best_params, best_score, best_model)
        else:
            return self.OUTPUT(trials_df, best_params, best_score, None)

def set_seed(seed=42):
    """Set all random seeds for reproducibility"""
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def cleanup_model_files(save_model_dir="save_model", max_trials_to_keep=5):
    """æ¸…ç†æ¨¡å‹æ–‡ä»¶ï¼Œåªä¿ç•™æœ€è¿‘çš„å‡ æ¬¡è¯•éªŒ"""
    if not os.path.exists(save_model_dir):
        return
    
    # è·å–æ‰€æœ‰è¯•éªŒç›®å½•
    trial_dirs = []
    for trial_dir in os.listdir(save_model_dir):
        trial_path = os.path.join(save_model_dir, trial_dir)
        if os.path.isdir(trial_path):
            # è·å–ç›®å½•çš„ä¿®æ”¹æ—¶é—´
            mtime = os.path.getmtime(trial_path)
            trial_dirs.append((mtime, trial_path))
    
    # æŒ‰æ—¶é—´æ’åº
    trial_dirs.sort(reverse=True)
    
    # åˆ é™¤æ—§çš„è¯•éªŒç›®å½•
    if len(trial_dirs) > max_trials_to_keep:
        for _, trial_path in trial_dirs[max_trials_to_keep:]:
            import shutil
            shutil.rmtree(trial_path)
        print(f"âœ… Cleaned up old trial models, keeping {max_trials_to_keep} most recent trials")

def clean_save_model_dir(save_model_dir="save_model"):
    """æ¸…ç©ºsave_modelç›®å½•"""
    if os.path.exists(save_model_dir):
        import shutil
        shutil.rmtree(save_model_dir)
        os.makedirs(save_model_dir)
        print(f"âœ… Cleaned up {save_model_dir} directory")

def compute_metrics(y_true, y_pred):
    """ä½¿ç”¨PyTorchè®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: é¢„æµ‹ç»“æœDataFrameï¼ŒåŒ…å«probabilityå’Œpredictionåˆ—
    
    Returns:
        dict: åŒ…å«å¤šä¸ªæŒ‡æ ‡çš„å­—å…¸
    """
    # å‡†å¤‡æ•°æ®
    prob_cols = [col for col in y_pred.columns if 'probability' in col]
    
    # æ£€æŸ¥æ¦‚ç‡åˆ—çš„æ•°é‡
    if len(prob_cols) == 1:
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ¦‚ç‡åˆ—ï¼Œåˆ›å»ºä¸¤ä¸ªç±»çš„æ¦‚ç‡
        probs = y_pred[prob_cols[0]].values
        probs = np.column_stack([1 - probs, probs])
    else:
        # å¦‚æœæœ‰å¤šä¸ªæ¦‚ç‡åˆ—ï¼Œç›´æ¥ä½¿ç”¨
        probs = y_pred[prob_cols].values
    
    # ç¡®ä¿å½¢çŠ¶æ­£ç¡®
    if len(probs.shape) == 1:
        probs = probs.reshape(-1, 1)
        probs = np.column_stack([1 - probs, probs])
    
    # ç¡®ä¿y_trueå’Œprobsçš„æ ·æœ¬æ•°é‡åŒ¹é…
    assert len(y_true) == len(probs), f"Sample size mismatch: y_true({len(y_true)}) != probs({len(probs)})"
    
    # è½¬æ¢ä¸ºtensor
    probs_tensor = torch.tensor(probs, dtype=torch.float32)
    target_tensor = torch.tensor(y_true.values, dtype=torch.long)
    
    # ç¡®ä¿target_tensoræ˜¯ä¸€ç»´çš„
    if len(target_tensor.shape) > 1:
        target_tensor = target_tensor.squeeze()
    
    # è®¡ç®—äº¤å‰ç†µæŸå¤±
    loss = F.cross_entropy(probs_tensor, target_tensor).item()
    
    # è®¡ç®—accuracy
    predictions = probs_tensor.argmax(dim=1)
    accuracy = (predictions == target_tensor).float().mean().item()
    
    # è®¡ç®—AUROC
    # ä½¿ç”¨æ­£ç±»çš„æ¦‚ç‡
    pos_probs = probs_tensor[:, 1]
    
    # å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç 
    target_one_hot = F.one_hot(target_tensor, num_classes=2).float()
    
    # è®¡ç®—TPRå’ŒFPR
    thresholds = torch.linspace(0, 1, steps=100)
    tpr = torch.zeros_like(thresholds)
    fpr = torch.zeros_like(thresholds)
    
    for i, threshold in enumerate(thresholds):
        y_pred = (pos_probs >= threshold).float()
        
        # True Positives
        tp = (y_pred * target_one_hot[:, 1]).sum()
        # False Positives
        fp = (y_pred * (1 - target_one_hot[:, 1])).sum()
        # True Negatives
        tn = ((1 - y_pred) * (1 - target_one_hot[:, 1])).sum()
        # False Negatives
        fn = ((1 - y_pred) * target_one_hot[:, 1]).sum()
        
        tpr[i] = tp / (tp + fn) if (tp + fn) > 0 else 0
        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    # æŒ‰ç…§FPRçš„é¡ºåºå¯¹TPRè¿›è¡Œæ’åº
    sorted_indices = torch.argsort(fpr)
    fpr = fpr[sorted_indices]
    tpr = tpr[sorted_indices]
    
    # ä½¿ç”¨æ¢¯å½¢æ³•åˆ™è®¡ç®—AUCï¼Œæ³¨æ„é¡ºåº
    auroc = torch.trapz(tpr, fpr).item()
    
    # ç¡®ä¿AUROCåœ¨[0,1]èŒƒå›´å†…
    auroc = max(0, min(1, auroc))
    
    return {
        'loss': loss,          # ä¸»è¦ä¼˜åŒ–æŒ‡æ ‡
        'auroc': auroc,        # æ¬¡è¦æŒ‡æ ‡
        'accuracy': accuracy   # æ¬¡è¦æŒ‡æ ‡
    }

def tune_selected_models(data_path):
    # Create results directory if it doesn't exist
    os.makedirs("results", exist_ok=True)
    
    # æ¸…ç©ºsave_modelç›®å½•
    clean_save_model_dir()
    
    # Set random seed
    set_seed(42)

    # Read data
    df = pd.read_excel(data_path)
    features = [c for c in df.columns if c.startswith("Feature")]
    X = df[features].copy()
    y = df["Label"].copy()

    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)    
    # Save the scaler
    joblib.dump(scaler, "results/scaler.pkl")
    print("âœ… Saved scaler to results/scaler.pkl")

    print("\nData Shape:", X.shape)
    print("Label Distribution:\n", y.value_counts())

    # Configure data
    data_config = DataConfig(
        target=['target'],
        continuous_cols=features,
        categorical_cols=[],
    )

    # é…ç½®è®­ç»ƒå™¨
    trainer_config = TrainerConfig(
        batch_size=1024,
        max_epochs=200,  # å¢åŠ è®­ç»ƒè½®æ•°
        early_stopping="valid_loss",
        early_stopping_patience=20,
        checkpoints="valid_loss",  # æ ¹æ®éªŒè¯é›†lossé€‰æ‹©æœ€ä½³æ¨¡å‹
        checkpoints_path="save_model",
        checkpoints_mode="min"  # lossè¶Šå°è¶Šå¥½
    )

    optimizer_config = OptimizerConfig(
        optimizer="Adam",
        optimizer_params={"lr": 0.001, "weight_decay": 0.01},
        lr_scheduler="ReduceLROnPlateau",
        lr_scheduler_params={"mode": "min", "patience": 5, "factor": 0.1},
        lr_scheduler_monitor_metric="valid_loss"
    )

    # Configure head with more options
    head_config = LinearHeadConfig(
        layers="256-128-64",  # æ·»åŠ æ›´å¤šå±‚
        dropout=0.2,
        initialization="kaiming",
        use_batch_norm=True
    ).__dict__

    # å®šä¹‰è¦è°ƒä¼˜çš„æ¨¡å‹é…ç½®
    model_configs = [
        # 1. DANet
        DANetConfig(
            task="classification",
            head="LinearHead",
            head_config=head_config,
            metrics=["accuracy", "auroc"],
            metrics_params=[{}, {}],
            metrics_prob_input=[False, True],
            n_layers=6,
            abstlay_dim_1=32,
            abstlay_dim_2=64,
            dropout_rate=0.1
        ),
        # 2. TabTransformer
        TabTransformerConfig(
            task="classification",
            head="LinearHead",
            head_config=head_config,
            metrics=["accuracy", "auroc"],
            metrics_params=[{}, {}],
            metrics_prob_input=[False, True],
            input_embed_dim=32,
            embedding_initialization="kaiming_uniform",
            embedding_bias=False,
            share_embedding=False,
            share_embedding_strategy="fraction",
            shared_embedding_fraction=0.25,
            num_heads=8,
            num_attn_blocks=6,
            embedding_dropout=0.1,
            transformer_head_dim=None,
            attn_dropout=0.1,
            add_norm_dropout=0.1,
            ff_dropout=0.1,
            ff_hidden_multiplier=4,
            transformer_activation="GEGLU",
            batch_norm_continuous_input=True
        ),
        # 3. AutoInt
        AutoIntConfig(
            task="classification",
            head="LinearHead",
            head_config=head_config,
            metrics=["accuracy", "auroc"],
            metrics_params=[{}, {}],
            metrics_prob_input=[False, True],
            embedding_dim=63,  # ä¸ layers çš„ç¬¬ä¸€å±‚ç»´åº¦åŒ¹é…
            attn_embed_dim=16,  # ä¸ layers çš„æœ€åä¸€å±‚ç»´åº¦åŒ¹é…
            # num_heads=2,
            num_attn_blocks=3,
            attn_dropouts=0.0,
            has_residuals=True,
            batch_norm_continuous_input=True,
            embedding_initialization="kaiming_uniform",
            embedding_bias=True,
            share_embedding=True,
            share_embedding_strategy="add",
            shared_embedding_fraction=0.25,
            deep_layers=False,
            # layers="63-24-16",  # ä»åµŒå…¥ç»´åº¦(32)åˆ°æ³¨æ„åŠ›ç»´åº¦(16)
            use_batch_norm=True,
            attention_pooling=True
        ),
        # 4. TabNet
        TabNetModelConfig(
            task="classification",
            head="LinearHead",
            head_config=head_config,
            metrics=["accuracy", "auroc"],
            metrics_params=[{}, {}],
            metrics_prob_input=[False, True],
            n_d=32,
            n_a=32,
            n_steps=3,
            gamma=1.5,
            n_independent=1,
            n_shared=2
        )
    ]

    # ä¸ºæ¯ä¸ªæ¨¡å‹å®šä¹‰æœç´¢ç©ºé—´
    search_spaces = [
        # DANet
        {
            "optimizer_config__optimizer": ["Adam", "AdamW"],
            "optimizer_config__optimizer_params": [
                {"weight_decay": 0.01},
                {"weight_decay": 0.001}
            ],
            "model_config__n_layers": [4, 6, 8],  # å¢åŠ å±‚æ•°é€‰é¡¹
            "model_config__abstlay_dim_1": [16, 32, 64],  # å¢åŠ ç»´åº¦é€‰é¡¹
            "model_config__abstlay_dim_2": [32, 64, 128],  # å¢åŠ ç»´åº¦é€‰é¡¹
            "model_config__k": [3, 5, 7],  # å¢åŠ  k å€¼é€‰é¡¹
            "model_config__dropout_rate": [0.0, 0.1, 0.2],  # å¢åŠ  dropout é€‰é¡¹
            "model_config__block_activation": ["ReLU", "LeakyReLU", "GELU"],  # å¢åŠ æ¿€æ´»å‡½æ•°é€‰é¡¹
            "model_config__virtual_batch_size": [128, 256, 512]  # å¢åŠ æ‰¹é‡å¤§å°é€‰é¡¹
        },
        # TabTransformer
        {
            "optimizer_config__optimizer": ["Adam", "AdamW"],
            "optimizer_config__optimizer_params": [
                {"weight_decay": 0.01},
                {"weight_decay": 0.001}
            ],
            "model_config__input_embed_dim": [16, 32, 64],  # å¢åŠ åµŒå…¥ç»´åº¦é€‰é¡¹
            "model_config__num_attn_blocks": [2, 4, 6],  # å¢åŠ æ³¨æ„åŠ›å—æ•°é‡
            "model_config__num_heads": [4, 8, 16],  # å¢åŠ å¤´æ•°é€‰é¡¹
            "model_config__transformer_head_dim": [None, 16, 32],  # å¢åŠ å¤´ç»´åº¦é€‰é¡¹
            "model_config__attn_dropout": [0.0, 0.1, 0.2],  # å¢åŠ æ³¨æ„åŠ›dropouté€‰é¡¹
            "model_config__add_norm_dropout": [0.0, 0.1, 0.2],  # å¢åŠ å½’ä¸€åŒ–dropouté€‰é¡¹
            "model_config__ff_dropout": [0.0, 0.1, 0.2],  # å¢åŠ å‰é¦ˆdropouté€‰é¡¹
            "model_config__ff_hidden_multiplier": [2, 4, 8],  # å¢åŠ éšè—å±‚å€æ•°é€‰é¡¹
            "model_config__transformer_activation": ["GEGLU", "ReGLU", "SwiGLU"],  # ä½¿ç”¨æ”¯æŒçš„æ¿€æ´»å‡½æ•°
            "model_config__embedding_initialization": ["kaiming_uniform", "kaiming_normal"],
            "model_config__embedding_bias": [True, False],
            "model_config__embedding_dropout": [0.0, 0.1, 0.2],
            "model_config__share_embedding": [True, False],
            "model_config__share_embedding_strategy": ["add", "fraction"],
            "model_config__shared_embedding_fraction": [0.25, 0.5, 0.75]
        },
        # AutoInt
        {
            "optimizer_config__optimizer": ["Adam", "AdamW"],
            "optimizer_config__optimizer_params": [
                {"weight_decay": 0.01},
                {"weight_decay": 0.001}
            ],
            "model_config__embedding_dim": [63, 32, 16],  # å›ºå®šä¸ºè¾“å…¥ç»´åº¦
            "model_config__attn_embed_dim": [16, 32, 64],  # æ³¨æ„åŠ›å±‚ç»´åº¦é€‰é¡¹
            "model_config__num_heads": [2, 4, 8],
            "model_config__num_attn_blocks": [2, 3, 4],
            "model_config__attn_dropouts": [0.0, 0.1, 0.2],
            "model_config__has_residuals": [True, False],
            "model_config__embedding_initialization": ["kaiming_uniform", "kaiming_normal"],
            "model_config__embedding_bias": [True, False],
            "model_config__share_embedding": [True, False],
            "model_config__share_embedding_strategy": ["add", "fraction"],
            "model_config__shared_embedding_fraction": [0.25, 0.5],
            "model_config__deep_layers": [False],
            "model_config__use_batch_norm": [True],
            "model_config__attention_pooling": [True, False],
            "model_config__activation": ["ReLU", "LeakyReLU"],  # æ·»åŠ æ¿€æ´»å‡½æ•°é€‰é¡¹
            "model_config__initialization": ["kaiming", "xavier"],  # æ·»åŠ åˆå§‹åŒ–æ–¹æ³•é€‰é¡¹
            "model_config__dropout": [0.0, 0.1, 0.2]  # æ·»åŠ dropouté€‰é¡¹
        },
        # TabNet
        {
            "optimizer_config__optimizer": ["Adam", "AdamW"],
            "optimizer_config__optimizer_params": [
                {"weight_decay": 0.01},
                {"weight_decay": 0.001}
            ],
            "model_config__n_d": [8, 16, 32, 64],  # é¢„æµ‹å±‚ç»´åº¦ (4-64)
            "model_config__n_a": [8, 16, 32, 64],  # æ³¨æ„åŠ›å±‚ç»´åº¦ (4-64)
            "model_config__n_steps": [3, 5, 7, 10],  # ç½‘ç»œæ­¥éª¤æ•° (3-10)
            "model_config__gamma": [1.0, 1.3, 1.5, 2.0],  # æ³¨æ„åŠ›æ›´æ–°çš„ç¼©æ”¾å› å­ (1.0-2.0)
            "model_config__n_independent": [1, 2, 3],  # ç‹¬ç«‹ GLU å±‚æ•°
            "model_config__n_shared": [1, 2, 3],  # å…±äº« GLU å±‚æ•°
            "model_config__virtual_batch_size": [128, 256, 512],  # Ghost Batch Normalization çš„æ‰¹é‡å¤§å°
            "model_config__mask_type": ["sparsemax", "entmax"],  # æ©ç å‡½æ•°ç±»å‹
            "model_config__batch_norm_continuous_input": [True],
            "model_config__embedding_dropout": [0.0, 0.1, 0.2]  # åµŒå…¥å±‚çš„ dropout ç‡
        }
    ]

    # åŠ è½½å†å²æœ€ä½³é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    best_configs_file = 'results/best_configs.json'
    historical_best = {}
    if os.path.exists(best_configs_file):
        try:
            with open(best_configs_file, 'r') as f:
                historical_best = json.load(f)
            print("\nLoaded historical best configurations")
        except json.JSONDecodeError:
            print("\nWarning: best_configs.json is corrupted, starting fresh")
            historical_best = {}

    print("\nStarting 10-Fold Cross-Validation Hyperparameter Tuning...")
    
    # åˆå§‹åŒ–10æŠ˜äº¤å‰éªŒè¯
    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    all_fold_results = []
    best_models = {}
    summary_results = []
    
    # å‡†å¤‡å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
    train_df = X.copy()
    train_df['target'] = y
    
    # æŒ‰é¡ºåºè°ƒä¼˜æ¯ä¸ªæ¨¡å‹
    model_names = ["DANetModel", "TabTransformerModel", "AutoIntModel", "TabNetModel"]
    
    for model_idx, (model_config, search_space) in enumerate(zip(model_configs, search_spaces)):
        model_name = model_names[model_idx]
        print(f"\n{'='*50}")
        print(f"Tuning {model_name}...")
        print(f"{'='*50}")
        
        # æ¯ä¸ªæ¨¡å‹å¼€å§‹å‰æ¸…ç©ºsave_modelç›®å½•
        clean_save_model_dir()
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹è®¾ç½®éšæœºç§å­
        set_seed(42)
        
        # ä½¿ç”¨æ–°çš„MultiMetricTabularModelTuneræ›¿ä»£åŸæ¥çš„TabularModelTuner
        tuner = MultiMetricTabularModelTuner(
            primary_metric="loss",  # ä½¿ç”¨lossä½œä¸ºä¸»è¦ä¼˜åŒ–æŒ‡æ ‡
            data_config=data_config,
            model_config=model_config,
            optimizer_config=optimizer_config,
            trainer_config=trainer_config
        )

        tuner_output = tuner.tune(
            train=train_df,
            search_space=search_space,
            strategy="random_search",
            n_trials=500,  # å¢åŠ åˆ°500æ¬¡å°è¯•
            metric=compute_metrics,  # ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡å‡½æ•°
            mode="min",    # æœ€å°åŒ–æŸå¤±
            cv=kfold,  # ä½¿ç”¨10æŠ˜äº¤å‰éªŒè¯
            cv_kwargs={"verbose": True},  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            progress_bar=True,
            verbose=True,
            return_best_model=True
        )
        
        # ä¿å­˜è°ƒä¼˜ç»“æœ
        tuner_output.trials_df['model'] = model_name
        all_fold_results.append(tuner_output.trials_df)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹åæ¸…ç†save_modelç›®å½•
        if tuner_output.best_model is not None:
            # ä¿å­˜æ¨¡å‹å‰ï¼Œæ¸…ç†æ—§çš„æ¨¡å‹æ–‡ä»¶
            model_save_path = f"results/{model_name}_best"
            if os.path.exists(model_save_path):
                import shutil
                shutil.rmtree(model_save_path)
            # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
            tuner_output.best_model.save_model(model_save_path)
            print(f"âœ… Saved best model to {model_save_path}")
            # æ¸…ç†save_modelç›®å½•
            clean_save_model_dir()
            
            # ä¿å­˜åˆ°æœ€ä½³æ¨¡å‹å­—å…¸
            best_models[model_name] = tuner_output.best_model
        
        # è·å–æœ€ä½³ç»“æœ
        best_trial = tuner_output.trials_df.sort_values("loss", ascending=True).iloc[0]
        print("\næœ€ä½³è¯•éªŒé…ç½®:")
        for col in best_trial.index:
            if col not in ['loss', 'auroc', 'accuracy', 'model', 'trial_id']:
                print(f"{col}: {best_trial[col]}")
        
        print(f"\n{model_name} æœ€ä½³äº¤å‰éªŒè¯ç»“æœ:")
        print(f"Loss: {best_trial['loss']:.4f}")
        print(f"AUROC: {best_trial['auroc']:.4f}")
        print(f"Accuracy: {best_trial['accuracy']:.4f}")
        
        # æ›´æ–°å†å²æœ€ä½³é…ç½®
        if model_name not in historical_best or best_trial['loss'] < historical_best[model_name]['performance'].get('loss', float('inf')):
            params_dict = {}
            for col in best_trial.index:
                if 'model_config__' in col or 'optimizer_config__' in col:
                    value = best_trial[col]
                    if isinstance(value, (np.integer, np.floating)):
                        value = value.item()
                    elif isinstance(value, np.bool_):
                        value = bool(value)
                    elif isinstance(value, dict):
                        value = {k: v.item() if isinstance(v, (np.integer, np.floating, np.bool_)) else v 
                               for k, v in value.items()}
                    params_dict[col] = value
            
            historical_best[model_name] = {
                'params': params_dict,
                'performance': {
                    'loss': float(best_trial['loss']),
                    'auroc': float(best_trial['auroc']),
                    'accuracy': float(best_trial['accuracy']),
                    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                }
            }
            print(f"\nğŸ† æ‰¾åˆ° {model_name} çš„æ–°æœ€ä½³é…ç½®!")
            print(f"Loss: {best_trial['loss']:.4f}")
            print(f"AUROC: {best_trial['auroc']:.4f}")
            print(f"Accuracy: {best_trial['accuracy']:.4f}")
        
        # æ·»åŠ åˆ°æ±‡æ€»ç»“æœ
        summary_results.append({
            "model_name": model_name,
            "best_loss": best_trial['loss'],
            "best_auroc": best_trial['auroc'],
            "best_accuracy": best_trial['accuracy'],
            "timestamp": historical_best[model_name]['performance']['timestamp']
        })
    
    # åˆå¹¶æ‰€æœ‰è°ƒä¼˜ç»“æœ
    combined_tuning_df = pd.concat(all_fold_results, ignore_index=True)
    combined_tuning_df.to_csv('results/all_cv_trials.csv', index=False)
    print("\nâœ… Saved all cross-validation trials to results/all_cv_trials.csv")
    
    # ä¿å­˜å†å²æœ€ä½³é…ç½®
    with open(best_configs_file, 'w') as f:
        json.dump(historical_best, f, indent=4)
    print("âœ… Saved historical best configurations to results/best_configs.json")
    
    # åˆ›å»ºæ±‡æ€»DataFrame
    summary_df = pd.DataFrame(summary_results)
    summary_df.to_csv('results/cv_summary.csv', index=False)
    print("âœ… Saved cross-validation summary to results/cv_summary.csv")
    
    # æ‰“å°æ±‡æ€»ç»“æœ
    print("=" * 80)
    print("\næ‰€æœ‰æ¨¡å‹çš„æœ€ä½³ç»“æœæ±‡æ€»:")
    for _, row in summary_df.iterrows():
        print(f"\næ¨¡å‹: {row['model_name']}")
        print(f"æœ€ä½³ Loss: {row['best_loss']:.4f}")
        print(f"æœ€ä½³ AUROC: {row['best_auroc']:.4f}")
        print(f"æœ€ä½³ Accuracy: {row['best_accuracy']:.4f}")
        print(f"è¾¾åˆ°æ—¶é—´: {row['timestamp']}")
    print("=" * 80)
    
    # è¿”å›æ‰€æœ‰ç»“æœ
    return pd.concat(all_fold_results), summary_df, best_models, historical_best

if __name__ == "__main__":
    data_path = "data/AI4healthcare.xlsx"
    tuner_df, summary_df, best_models, historical_best = tune_selected_models(data_path) 